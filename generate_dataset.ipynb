{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d8938758-9ce0-4a7f-bf50-4ce3543600c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd065fad-d07b-411b-a93d-e880592f95a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-28 02:17:09.296228: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-28 02:17:09.524663: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738030629.640539    2945 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738030629.673492    2945 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-28 02:17:09.890318: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Downloading shards: 100%|██████████| 2/2 [06:28<00:00, 194.07s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.27s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac1d3015-989f-4de1-b582-521230462a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "195df160-e973-434c-83de-b42b97954f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7562b7f8-33ef-4d8d-9d86-72fb299c0927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|██████████| 4358/4358 [00:00<00:00, 163290.84 examples/s]\n",
      "Generating train split: 100%|██████████| 36718/36718 [00:00<00:00, 321960.90 examples/s]\n",
      "Generating validation split: 100%|██████████| 3760/3760 [00:00<00:00, 252030.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9df9b17-87f3-433b-bfd5-8d0cf2f7395e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4358/4358 [00:00<00:00, 4433.20 examples/s]\n",
      "Map: 100%|██████████| 36718/36718 [00:05<00:00, 6471.21 examples/s]\n",
      "Map: 100%|██████████| 3760/3760 [00:00<00:00, 7254.66 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37b8b2a9-3b56-486d-86d8-9a44b7290669",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiTextDataset(Dataset):\n",
    "    def __init__(self, tokenized_dataset):\n",
    "        self.input_ids = tokenized_dataset[\"input_ids\"]\n",
    "        self.attention_mask = tokenized_dataset[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# Create PyTorch datasets for each split\n",
    "train_dataset = WikiTextDataset(tokenized_datasets[\"train\"])\n",
    "val_dataset = WikiTextDataset(tokenized_datasets[\"validation\"])\n",
    "test_dataset = WikiTextDataset(tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d36e1961-35a3-4b12-b23e-b83c7af43b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak GPU memory usage: 30.00 GB\n"
     ]
    }
   ],
   "source": [
    "model.zero_grad()\n",
    "\n",
    "# Create a dummy input\n",
    "dummy_text = \"Say only one word: butterfly.\"\n",
    "inputs = tokenizer(dummy_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "# Perform inference and measure memory usage\n",
    "with torch.no_grad():\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    outputs = model(**inputs)\n",
    "    peak_memory = torch.cuda.max_memory_allocated(device) / 1024**3  # Convert to GB\n",
    "    print(f\"Peak GPU memory usage: {peak_memory:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6fdd20cb-1784-46f8-b124-6cec57162d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"residual_stream_dataset\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3132faca-0889-4ea2-aea2-38894b1ed866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook function to capture residual stream\n",
    "residual_stream = None\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    global residual_stream\n",
    "    residual_stream = output\n",
    "\n",
    "# Register the hook for the 16th transformer layer (index 15)\n",
    "layer_to_hook = 15\n",
    "hook_handle = model.model.layers[layer_to_hook].register_forward_hook(hook_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "40459088-a7fb-4b80-b93e-0ed5b11dc68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process batches\n",
    "def process_batches(data_loader, output_file):\n",
    "    global residual_stream\n",
    "    residual_vectors = []\n",
    "\n",
    "    with tqdm(total=len(data_loader), desc=\"Processing batches\") as pbar:\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            # Move inputs to device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    \n",
    "            # Run the model\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "            # Extract residual stream vectors\n",
    "            if residual_stream is not None:\n",
    "                # residual_stream: [batch_size, seq_len, 4096]\n",
    "                res = residual_stream[0]\n",
    "                for i in range(res.size(0)):  # Iterate over batch\n",
    "                    valid_vectors = res[i][attention_mask[i].bool()]  # Remove padding\n",
    "                    residual_vectors.append(valid_vectors.cpu().numpy())\n",
    "            \n",
    "            # Save in chunks to avoid memory issues\n",
    "            if batch_idx % 10 == 0 and batch_idx > 0:\n",
    "                save_vectors(residual_vectors, output_file, mode='a')  # Append mode\n",
    "                residual_vectors.clear()  # Clear buffer\n",
    "    \n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Save remaining vectors\n",
    "    save_vectors(residual_vectors, output_file, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e5bc8d30-f1fe-4758-b367-93efd735e96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vectors(vectors, output_file, mode='w'):\n",
    "    if not vectors:\n",
    "        return\n",
    "    # Flatten nested list of NumPy arrays\n",
    "    vectors = np.vstack(vectors)\n",
    "    if mode == 'w':  # Write mode\n",
    "        np.save(output_file, vectors)\n",
    "    elif mode == 'a':  # Append mode\n",
    "        with open(output_file, 'ab') as f:\n",
    "            np.save(f, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "80a8fc26-7ca2-4567-a052-897b7cd49440",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "52121ef4-5e05-43f2-9943-0c213f6573b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "53f43f55-050e-44d3-a6b6-381752d1ec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ad2a0d8c-5715-46c7-b5b7-27b056b6bfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|▏         | 164/7344 [03:11<2:19:31,  1.17s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2945/2441449548.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresidual_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Process train_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprocess_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_residuals.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2945/4073631999.py\u001b[0m in \u001b[0;36mprocess_batches\u001b[0;34m(data_loader, output_file)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Iterate over batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                     \u001b[0mvalid_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Remove padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                     \u001b[0mresidual_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# Save in chunks to avoid memory issues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "residual_stream = None\n",
    "# Process train_loader\n",
    "process_batches(train_loader, os.path.join(output_dir, \"train_residuals.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c822c1-2bd3-447a-b997-f22ef1f06caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_stream = None\n",
    "# Process val_loader\n",
    "process_batches(val_loader, os.path.join(output_dir, \"val_residuals.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e383723-798f-4ec3-b262-b17b5a8a9d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_stream = None\n",
    "# Process test_loader\n",
    "process_batches(test_loader, os.path.join(output_dir, \"test_residuals.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1e8631-e9e5-4d07-88e2-827478311af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the hook after processing\n",
    "hook_handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "22154b8d-bf2d-466c-9dc7-b2d56556fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"residual_stream_dataset/train_residuals.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "31e9a723-5d20-4bcd-869f-dbf23f6bbafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2036, 4096)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05b4456-ca6b-4e38-948b-4f2a53e6dbb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b6ce96-8690-465f-a9e5-4d8291b37789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeebe59-65d7-4cd0-8a4e-936746e2051a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20133ed-b6f2-4347-964e-a182ee4d716c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecd3a42-f110-4ca1-9f39-05ee976a4cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d52a776d-f17a-4074-9514-d9677dc0fbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: The quick brown fox jumps over the lazy dog. The lazy caterpillar goes to the green garden. The boy with the big backpack walks past the little girl with the red hat. The man with the blue shirt buys a new kite. The red balloon floats in the sky\n"
     ]
    }
   ],
   "source": [
    "model.zero_grad()\n",
    "\n",
    "# Example input prompt\n",
    "prompt = \"The quick brown fox\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")  # Move to GPU if available\n",
    "\n",
    "# Initialize variables\n",
    "generated_ids = inputs[\"input_ids\"]  # Start with the tokenized input\n",
    "max_new_tokens = 50  # Limit for generation\n",
    "eos_token_id = tokenizer.eos_token_id  # EOS token ID\n",
    "temperature = 0.7  # Set desired temperature\n",
    "\n",
    "# Loop for generation\n",
    "for _ in range(max_new_tokens):\n",
    "    # Generate logits\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=generated_ids)\n",
    "    next_token_logits = outputs.logits[:, -1, :]  # Shape: (batch_size, vocab_size)\n",
    "    \n",
    "    # Apply temperature scaling\n",
    "    next_token_logits = next_token_logits / temperature\n",
    "    \n",
    "    # Convert logits to probabilities\n",
    "    probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "    \n",
    "    # Sample the next token\n",
    "    next_token_id = torch.multinomial(probabilities, num_samples=1)  # Sample from the distribution\n",
    "    \n",
    "    # Append the new token to the sequence\n",
    "    generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
    "    \n",
    "    # Check if EOS token is generated\n",
    "    if eos_token_id is not None and next_token_id.item() == eos_token_id:\n",
    "        print(\"EOS token generated. Stopping generation.\")\n",
    "        break\n",
    "\n",
    "# Decode the full generated text\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "746946b6-7e0d-457c-b5e7-0e8c3c029cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_stream = None  # Variable to store the hidden state\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    global residual_stream\n",
    "    residual_stream = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "39867fc6-95e1-4d74-8816-eceabaa7950c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaDecoderLayer(\n",
       "  (self_attn): LlamaAttention(\n",
       "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "  )\n",
       "  (mlp): LlamaMLP(\n",
       "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "    (act_fn): SiLU()\n",
       "  )\n",
       "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1910fe09-50a2-4b6d-a760-d07e58941882",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_to_hook = 15  # 16th layer\n",
    "hook_handle = model.model.layers[layer_to_hook].register_forward_hook(hook_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "56f3772b-59d9-4272-bb3e-571ea9d5010a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak GPU memory usage: 32.27 GB\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy input\n",
    "dummy_texts = [str(x) for x in range(1024)]\n",
    "inputs = tokenizer(dummy_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "# Perform inference and measure memory usage\n",
    "with torch.no_grad():\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    outputs = model(**inputs)\n",
    "    peak_memory = torch.cuda.max_memory_allocated(device) / 1024**3  # Convert to GB\n",
    "    print(f\"Peak GPU memory usage: {peak_memory:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f9c80107-8e5c-49c8-b340-079a6cda024d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 11, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(residual_stream[0].shape)  # Expected shape: (batch_size, seq_len, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "076f42d1-5957-485a-8b71-c7cb69d6d37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[128000,     40,   1097,   1097,   1097,   1097,   1097,   1097,   1097,\n",
      "           1097,     13],\n",
      "        [128001, 128001, 128001, 128001, 128001, 128000,    791,  13180,    374,\n",
      "           6437,     13]], device='cuda:0')\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "Pad Token ID: 128001\n"
     ]
    }
   ],
   "source": [
    "print(\"Input IDs:\", inputs[\"input_ids\"])  # Tokenized input IDs, including padding tokens\n",
    "print(\"Attention Mask:\", inputs[\"attention_mask\"])  # Mask where 1 indicates real tokens, 0 indicates padding\n",
    "print(\"Pad Token ID:\", tokenizer.pad_token_id)  # ID used for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dc5d253f-e933-490a-8e22-06685dfd8e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak GPU memory usage: 30.28 GB\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy input\n",
    "dummy_texts = [\"I.\", \"The sky is blue.\", \"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\",\"3\"]\n",
    "inputs = tokenizer(dummy_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "# Perform inference and measure memory usage\n",
    "with torch.no_grad():\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    outputs = model(**inputs)\n",
    "    peak_memory = torch.cuda.max_memory_allocated(device) / 1024**3  # Convert to GB\n",
    "    print(f\"Peak GPU memory usage: {peak_memory:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "771f4fc9-0413-4c56-bc10-cfe041431f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 6, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(residual_stream[0].shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5bf7ccde-8d73-4b23-9ef0-51f2152a59b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 1            |        cudaMalloc retries: 4         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  30719 MiB |  31011 MiB | 231310 MiB | 200591 MiB |\n",
      "|       from large pool |  30698 MiB |  30989 MiB | 155812 MiB | 125114 MiB |\n",
      "|       from small pool |     20 MiB |     28 MiB |  75498 MiB |  75477 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  30719 MiB |  31011 MiB | 231310 MiB | 200591 MiB |\n",
      "|       from large pool |  30698 MiB |  30989 MiB | 155812 MiB | 125114 MiB |\n",
      "|       from small pool |     20 MiB |     28 MiB |  75498 MiB |  75477 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  30719 MiB |  31008 MiB | 227512 MiB | 196793 MiB |\n",
      "|       from large pool |  30698 MiB |  30986 MiB | 152034 MiB | 121336 MiB |\n",
      "|       from small pool |     20 MiB |     28 MiB |  75478 MiB |  75457 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  80386 MiB |  80386 MiB |  80640 MiB | 260096 KiB |\n",
      "|       from large pool |  80308 MiB |  80308 MiB |  80556 MiB | 253952 KiB |\n",
      "|       from small pool |     78 MiB |     78 MiB |     84 MiB |   6144 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  25359 KiB | 107407 KiB | 136343 MiB | 136319 MiB |\n",
      "|       from large pool |  18178 KiB | 100226 KiB |  56104 MiB |  56086 MiB |\n",
      "|       from small pool |   7181 KiB |   9165 KiB |  80239 MiB |  80232 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     436    |     513    |  305456    |  305020    |\n",
      "|       from large pool |     229    |     297    |   22281    |   22052    |\n",
      "|       from small pool |     207    |     220    |  283175    |  282968    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     436    |     513    |  305456    |  305020    |\n",
      "|       from large pool |     229    |     297    |   22281    |   22052    |\n",
      "|       from small pool |     207    |     220    |  283175    |  282968    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     748    |     748    |     762    |      14    |\n",
      "|       from large pool |     709    |     709    |     720    |      11    |\n",
      "|       from small pool |      39    |      39    |      42    |       3    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      34    |      56    |  162276    |  162242    |\n",
      "|       from large pool |       3    |      25    |    9488    |    9485    |\n",
      "|       from small pool |      31    |      34    |  152788    |  152757    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194c2de0-bc44-45d3-9240-5979c544ba06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
